{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0p2SV2zQxt4_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yi9W8_yl8ai",
        "outputId": "40f562e3-5fe7-41f7-ef30-4b0eee9730cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "input_file_segments = '/content/drive/MyDrive/Project/ExtractedSegments_1s_set4.npy'\n",
        "input_file_labels = '/content/drive/MyDrive/Project/ExtractedLabelas_1s_set4.npy'\n",
        "\n",
        "loaded_segments = np.load(input_file_segments, allow_pickle=True)\n",
        "loaded_labels = np.load(input_file_labels, allow_pickle=True)\n",
        "\n",
        "print(f'Total segments loaded: {len(loaded_segments)}')\n",
        "print(f'Total labels loaded: {len(loaded_labels)}')\n",
        "print(f'Sample segment shape: {loaded_segments[0].shape}')\n",
        "print(f'Sample label: {loaded_labels[0]}')\n",
        "\n",
        "label_counts = Counter(loaded_labels)\n",
        "print(f'Label counts: {label_counts}')\n",
        "\n",
        "max_count = max(label_counts.values())\n",
        "\n",
        "def add_gaussian_noise(data, mean=0, std_dev=0.05):\n",
        "    noise = np.random.normal(mean, std_dev, data.shape)\n",
        "    return data + noise\n",
        "\n",
        "augmented_segments = []\n",
        "augmented_labels = []\n",
        "\n",
        "for segment, label in zip(loaded_segments, loaded_labels):\n",
        "    if segment.shape == (125, 16):\n",
        "        augmented_segments.append(segment)\n",
        "        augmented_labels.append(label)\n",
        "    else:\n",
        "        print(f\"Skipping segment with invalid shape: {segment.shape}\")\n",
        "\n",
        "for label, count in label_counts.items():\n",
        "    if count < max_count:\n",
        "        label_segments = [seg for seg, lbl in zip(loaded_segments, loaded_labels) if lbl == label]\n",
        "\n",
        "        num_samples_needed = max_count - count\n",
        "\n",
        "        for _ in range(num_samples_needed):\n",
        "            segment = label_segments[np.random.randint(len(label_segments))]\n",
        "            noisy_segment = add_gaussian_noise(segment)\n",
        "\n",
        "            if noisy_segment.shape == (125, 16):\n",
        "                augmented_segments.append(noisy_segment)\n",
        "                augmented_labels.append(label)\n",
        "            else:\n",
        "                print(f\"Generated segment with invalid shape: {noisy_segment.shape}\")\n",
        "\n",
        "augmented_segments = np.array(augmented_segments)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "print(f'Augmented data shape: {augmented_segments.shape}')\n",
        "print(f'Augmented labels shape: {augmented_labels.shape}')\n",
        "print(f'New label counts: {Counter(augmented_labels)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_BPl2HmmBIy",
        "outputId": "1427e72e-8ea1-43b4-e36b-02bac029c75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total segments loaded: 937\n",
            "Total labels loaded: 937\n",
            "Sample segment shape: (125, 16)\n",
            "Sample label: Walking\n",
            "Label counts: Counter({'Walking': 290, 'Aha': 290, 'Doing Other Task': 290, 'Impasse': 42, 'Re-evaluation': 25})\n",
            "Augmented data shape: (1450, 125, 16)\n",
            "Augmented labels shape: (1450,)\n",
            "New label counts: Counter({'Walking': 290, 'Aha': 290, 'Doing Other Task': 290, 'Impasse': 290, 'Re-evaluation': 290})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = augmented_segments\n",
        "final_labels = augmented_labels"
      ],
      "metadata": {
        "id": "E3lam83emFZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "yqcGQwOQx1Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_mean(data):\n",
        "    return np.mean(data, axis=0)\n",
        "\n",
        "mean_features = np.array([compute_mean(subject_data) for subject_data in data])\n",
        "\n",
        "def compute_variance(data):\n",
        "    return np.var(data, axis=0)\n",
        "\n",
        "variance_features = np.array([compute_variance(subject_data) for subject_data in data])\n",
        "\n",
        "def compute_first_diff(data):\n",
        "    return np.mean(np.abs(np.diff(data, axis=0)), axis=0)\n",
        "\n",
        "first_diff_features = np.array([compute_first_diff(subject_data) for subject_data in data])\n",
        "\n",
        "def compute_second_diff(data):\n",
        "    return np.mean(np.abs(np.diff(data, n=2, axis=0)), axis=0)\n",
        "\n",
        "second_diff_features = np.array([compute_second_diff(subject_data) for subject_data in data])\n",
        "\n",
        "combined_time_features = np.concatenate(\n",
        "    [mean_features, variance_features, first_diff_features, second_diff_features], axis=1)"
      ],
      "metadata": {
        "id": "Kja-_0wd1yDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.signal import spectrogram\n",
        "\n",
        "\n",
        "def extract_frequency_features(segments):\n",
        "\n",
        "    feature_vectors = []\n",
        "\n",
        "    for segment in segments:\n",
        "\n",
        "        channel_features = []\n",
        "\n",
        "\n",
        "        for channel_data in segment.T:\n",
        "\n",
        "            freqs, times, Sxx = spectrogram(channel_data, fs=125, nperseg=16)\n",
        "\n",
        "            Sxx_magnitude = np.abs(Sxx)\n",
        "\n",
        "            mean_power = np.mean(Sxx_magnitude)\n",
        "            std_power = np.std(Sxx_magnitude)\n",
        "            peak_freq = freqs[np.argmax(np.mean(Sxx_magnitude, axis=1))]\n",
        "\n",
        "            channel_features.extend([mean_power, std_power, peak_freq])\n",
        "\n",
        "\n",
        "        feature_vectors.append(channel_features)\n",
        "\n",
        "    return np.array(feature_vectors)\n",
        "\n",
        "frequency_features = extract_frequency_features(data)\n",
        "\n",
        "print(frequency_features.shape)"
      ],
      "metadata": {
        "id": "2ulF4v_8w99q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb40c6c9-c9ba-41f2-f9d2-3e2fa3aeec7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1450, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.signal import spectrogram\n",
        "\n",
        "\n",
        "def extract_frequency_features_full_spectrum(segments):\n",
        "    feature_vectors = []\n",
        "\n",
        "    for segment in segments:\n",
        "        channel_features = []\n",
        "\n",
        "        for channel_data in segment.T:\n",
        "            freqs, times, Sxx = spectrogram(channel_data, fs=125, nperseg=16)\n",
        "\n",
        "            Sxx_magnitude = np.abs(Sxx)\n",
        "\n",
        "            mean_power = np.mean(Sxx_magnitude)\n",
        "\n",
        "            std_power = np.std(Sxx_magnitude)\n",
        "\n",
        "            peak_freq = freqs[np.argmax(np.mean(Sxx_magnitude, axis=1))]\n",
        "\n",
        "            flat_Sxx = Sxx_magnitude.mean(axis=1)\n",
        "            lower_freq = freqs[np.searchsorted(np.cumsum(flat_Sxx), 0.25 * np.sum(flat_Sxx))]\n",
        "            upper_freq = freqs[np.searchsorted(np.cumsum(flat_Sxx), 0.75 * np.sum(flat_Sxx))]\n",
        "            bandwidth = upper_freq - lower_freq\n",
        "\n",
        "            channel_features.extend([mean_power, std_power, peak_freq, bandwidth])\n",
        "\n",
        "        feature_vectors.append(channel_features)\n",
        "\n",
        "    return np.array(feature_vectors)\n",
        "\n",
        "frequency_features_full = extract_frequency_features_full_spectrum(data)\n",
        "\n",
        "print(frequency_features_full.shape)"
      ],
      "metadata": {
        "id": "uwXKZ-J_xB0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb697ee-f9ed-4f5e-d1ac-d14b6f2a8f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1450, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets"
      ],
      "metadata": {
        "id": "VD2gewQtxGjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36164d8b-9280-49b9-dfcc-31d74fecfe16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from PyWavelets) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import pywt\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "\n",
        "def extract_ar_coefficients(data, order=1):\n",
        "    ar_coeffs = []\n",
        "    for segment in data:\n",
        "        if len(segment.shape) > 1:\n",
        "            segment = segment.flatten()\n",
        "        model = AutoReg(segment, lags=order).fit()\n",
        "        ar_coeffs.append(model.params)\n",
        "\n",
        "    return np.array(ar_coeffs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_wavelet_variance(data, wavelet='db2', level=5, fixed_length=5):\n",
        "    wavelet_variances = []\n",
        "    for segment in data:\n",
        "        max_level = pywt.dwt_max_level(len(segment), wavelet)\n",
        "        coeffs = pywt.wavedec(segment, wavelet, level=min(level, max_level))\n",
        "\n",
        "        variances = [np.var(c) for c in coeffs]\n",
        "\n",
        "        if len(variances) < fixed_length:\n",
        "            variances = np.pad(variances, (0, fixed_length - len(variances)), 'constant')\n",
        "        else:\n",
        "            variances = variances[:fixed_length]\n",
        "\n",
        "        wavelet_variances.append(variances)\n",
        "\n",
        "    return np.array(wavelet_variances)\n",
        "\n",
        "def extract_features(data, ar_order=1, wavelet='db2', level=5, fixed_length=5):\n",
        "    ar_features = extract_ar_coefficients(data, order=ar_order)\n",
        "    wavelet_var_features = extract_wavelet_variance(data, wavelet=wavelet, level=level, fixed_length=fixed_length)\n",
        "\n",
        "    combined_features = []\n",
        "    for ar,  wv in zip(ar_features, wavelet_var_features):\n",
        "        combined = np.hstack((ar, wv))\n",
        "        combined_features.append(combined)\n",
        "\n",
        "    return np.array(combined_features)\n",
        "\n",
        "wv_features = extract_features(data)\n",
        "\n",
        "print(f'Features matrix shape: {wv_features.shape}')"
      ],
      "metadata": {
        "id": "U1FHEifXxG_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0935786-b055-40b9-8881-dcc96bb8aa00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pywt/_multilevel.py:43: UserWarning: Level value of 5 is too high: all coefficients will experience boundary effects.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features matrix shape: (1450, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.signal import welch\n",
        "\n",
        "def extract_frequency_features(data, sfreq):\n",
        "    n_samples, n_times, n_channels = data.shape\n",
        "    freq_bands = {\n",
        "        'delta': (0.5, 4),\n",
        "        'theta': (4, 8),\n",
        "        'alpha/mu': (8, 12),\n",
        "        'low_alpha': (8, 10),\n",
        "        'high_alpha': (10, 12),\n",
        "        'beta': (12, 30),\n",
        "        'low_beta': (12, 15),\n",
        "        'mid_beta': (15, 20),\n",
        "        'high_beta': (20, 30),\n",
        "        'gamma': (30, 100),\n",
        "        'low_gamma': (30, 50),\n",
        "        'high_gamma': (50, 100),\n",
        "        'epsilon': (0.1, 0.5),\n",
        "        'sigma': (12, 16),\n",
        "        'high_frequency_oscillations': (100, 500),\n",
        "        'ripples': (80, 200),\n",
        "        'fast_ripples': (200, 500)\n",
        "    }\n",
        "\n",
        "    freq_features = np.zeros((n_samples, n_channels, len(freq_bands)))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_channels):\n",
        "            f, psd = welch(data[i, j, :], sfreq, nperseg=min(256, n_times))\n",
        "\n",
        "            for band, (f_low, f_high) in freq_bands.items():\n",
        "                idx_band = np.where((f >= f_low) & (f < f_high))[0]\n",
        "                if len(idx_band) > 0:\n",
        "                    power_in_band = np.mean(psd[idx_band])\n",
        "                else:\n",
        "                    power_in_band = 0.0\n",
        "                freq_features[i, j, list(freq_bands.keys()).index(band)] = power_in_band\n",
        "\n",
        "    return freq_features\n",
        "\n",
        "n_subjects, n_times, n_channels = data.shape\n",
        "reshaped_data = data.reshape(n_subjects, n_times, n_channels)\n",
        "X_freq_features = extract_frequency_features(reshaped_data, sfreq=125)\n",
        "\n",
        "print('Shape of X_freq_features:', X_freq_features.shape)"
      ],
      "metadata": {
        "id": "hDkiDIpkxOlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efeffb31-d288-4705-8d0b-05c6d53d6c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 125 is greater than input length  = 16, using nperseg = 16\n",
            "  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n",
            "<ipython-input-46-5ca8bf3279b1>:41: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  freq_features[i, j, list(freq_bands.keys()).index(band)] = power_in_band\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_freq_features: (1450, 16, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(data):\n",
        "    mean = np.mean(data, axis=0)\n",
        "    std = np.std(data, axis=0)\n",
        "    std[std == 0] = 1\n",
        "\n",
        "    standardized_data = (data - mean) / std\n",
        "    return standardized_data\n",
        "\n",
        "\n",
        "X_standardized_freq_features = standardize_data(X_freq_features)\n",
        "print(\"Standardized data shape:\", X_standardized_freq_features.shape)"
      ],
      "metadata": {
        "id": "SbAt_YAXxPNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05132fc-340c-48a2-e514-cea77102b082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized data shape: (1450, 16, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_data_freq_features = X_standardized_freq_features.reshape(X_standardized_freq_features.shape[0], 16 * 17)\n",
        "\n",
        "print(flattened_data_freq_features.shape)"
      ],
      "metadata": {
        "id": "ksFqKzI2xTuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3741b529-dc7a-42e6-a97c-4c5e003f9f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1450, 272)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier"
      ],
      "metadata": {
        "id": "cyUe_JuVx5-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(final_labels)\n",
        "\n",
        "concatenated_array = np.concatenate([flattened_data_freq_features,wv_features,frequency_features_full,frequency_features, combined_time_features], axis=1)\n",
        "concatenated_array_features = np.array(concatenated_array, dtype=np.float32)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(concatenated_array_features, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "classifiers = {\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Naive Bayes\": GaussianNB()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1\n",
        "    }\n",
        "\n",
        "    print(f\"Results for {name}:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"\")\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nSummary of Classifier Performance:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mub9wiG2u9Tx",
        "outputId": "8da71ac8-590a-47f7-e5a3-31444acf5f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for SVM:\n",
            "Accuracy: 0.3310\n",
            "Precision: 0.2945\n",
            "Recall: 0.3310\n",
            "F1 Score: 0.2707\n",
            "\n",
            "Results for Random Forest:\n",
            "Accuracy: 0.4862\n",
            "Precision: 0.4588\n",
            "Recall: 0.4862\n",
            "F1 Score: 0.4693\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:47:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for XGBoost:\n",
            "Accuracy: 0.5069\n",
            "Precision: 0.4816\n",
            "Recall: 0.5069\n",
            "F1 Score: 0.4921\n",
            "\n",
            "Results for Gradient Boosting:\n",
            "Accuracy: 0.5310\n",
            "Precision: 0.5145\n",
            "Recall: 0.5310\n",
            "F1 Score: 0.5201\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for AdaBoost:\n",
            "Accuracy: 0.2966\n",
            "Precision: 0.3190\n",
            "Recall: 0.2966\n",
            "F1 Score: 0.3033\n",
            "\n",
            "Results for K-Nearest Neighbors:\n",
            "Accuracy: 0.4966\n",
            "Precision: 0.4482\n",
            "Recall: 0.4966\n",
            "F1 Score: 0.4488\n",
            "\n",
            "Results for Logistic Regression:\n",
            "Accuracy: 0.3828\n",
            "Precision: 0.3369\n",
            "Recall: 0.3828\n",
            "F1 Score: 0.3345\n",
            "\n",
            "Results for Naive Bayes:\n",
            "Accuracy: 0.2069\n",
            "Precision: 0.3508\n",
            "Recall: 0.2069\n",
            "F1 Score: 0.1596\n",
            "\n",
            "\n",
            "Summary of Classifier Performance:\n",
            "                     Accuracy  Precision    Recall  F1 Score\n",
            "SVM                  0.331034   0.294520  0.331034  0.270730\n",
            "Random Forest        0.486207   0.458799  0.486207  0.469257\n",
            "XGBoost              0.506897   0.481638  0.506897  0.492112\n",
            "Gradient Boosting    0.531034   0.514507  0.531034  0.520132\n",
            "AdaBoost             0.296552   0.318973  0.296552  0.303315\n",
            "K-Nearest Neighbors  0.496552   0.448178  0.496552  0.448762\n",
            "Logistic Regression  0.382759   0.336937  0.382759  0.334524\n",
            "Naive Bayes          0.206897   0.350755  0.206897  0.159623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}