{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yi9W8_yl8ai",
        "outputId": "abb4dbc4-3f7a-433b-b7b4-50091b10693c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "input_file_segments = '/content/drive/MyDrive/Project/ExtractedSegments_1s_set4.npy'\n",
        "input_file_labels = '/content/drive/MyDrive/Project/ExtractedLabelas_1s_set4.npy'\n",
        "\n",
        "loaded_segments = np.load(input_file_segments, allow_pickle=True)\n",
        "loaded_labels = np.load(input_file_labels, allow_pickle=True)\n",
        "\n",
        "print(f'Total segments loaded: {len(loaded_segments)}')\n",
        "print(f'Total labels loaded: {len(loaded_labels)}')\n",
        "print(f'Sample segment shape: {loaded_segments[0].shape}')\n",
        "print(f'Sample label: {loaded_labels[0]}')\n",
        "\n",
        "label_counts = Counter(loaded_labels)\n",
        "print(f'Label counts: {label_counts}')\n",
        "\n",
        "max_count = max(label_counts.values())\n",
        "\n",
        "def add_gaussian_noise(data, mean=0, std_dev=0.05):\n",
        "    noise = np.random.normal(mean, std_dev, data.shape)\n",
        "    return data + noise\n",
        "\n",
        "augmented_segments = []\n",
        "augmented_labels = []\n",
        "\n",
        "for segment, label in zip(loaded_segments, loaded_labels):\n",
        "    if segment.shape == (125, 16):\n",
        "        augmented_segments.append(segment)\n",
        "        augmented_labels.append(label)\n",
        "    else:\n",
        "        print(f\"Skipping segment with invalid shape: {segment.shape}\")\n",
        "\n",
        "for label, count in label_counts.items():\n",
        "    if count < max_count:\n",
        "        label_segments = [seg for seg, lbl in zip(loaded_segments, loaded_labels) if lbl == label]\n",
        "\n",
        "        num_samples_needed = max_count - count\n",
        "\n",
        "        for _ in range(num_samples_needed):\n",
        "            segment = label_segments[np.random.randint(len(label_segments))]\n",
        "            noisy_segment = add_gaussian_noise(segment)\n",
        "\n",
        "            if noisy_segment.shape == (125, 16):\n",
        "                augmented_segments.append(noisy_segment)\n",
        "                augmented_labels.append(label)\n",
        "            else:\n",
        "                print(f\"Generated segment with invalid shape: {noisy_segment.shape}\")\n",
        "\n",
        "augmented_segments = np.array(augmented_segments)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "print(f'Augmented data shape: {augmented_segments.shape}')\n",
        "print(f'Augmented labels shape: {augmented_labels.shape}')\n",
        "print(f'New label counts: {Counter(augmented_labels)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_BPl2HmmBIy",
        "outputId": "c93e0b58-198e-4cfe-d4ef-013b29c685f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total segments loaded: 937\n",
            "Total labels loaded: 937\n",
            "Sample segment shape: (125, 16)\n",
            "Sample label: Walking\n",
            "Label counts: Counter({'Walking': 290, 'Aha': 290, 'Doing Other Task': 290, 'Impasse': 42, 'Re-evaluation': 25})\n",
            "Augmented data shape: (1450, 125, 16)\n",
            "Augmented labels shape: (1450,)\n",
            "New label counts: Counter({'Walking': 290, 'Aha': 290, 'Doing Other Task': 290, 'Impasse': 290, 'Re-evaluation': 290})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = augmented_segments\n",
        "final_labels = augmented_labels"
      ],
      "metadata": {
        "id": "E3lam83emFZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_mean(data):\n",
        "    return np.mean(data, axis=0)\n",
        "\n",
        "mean_features = np.array([compute_mean(subject_data) for subject_data in data])\n",
        "\n",
        "def compute_variance(data):\n",
        "    return np.var(data, axis=0)\n",
        "\n",
        "variance_features = np.array([compute_variance(subject_data) for subject_data in data])\n",
        "\n",
        "def compute_first_diff(data):\n",
        "    return np.mean(np.abs(np.diff(data, axis=0)), axis=0)\n",
        "\n",
        "first_diff_features = np.array([compute_first_diff(subject_data) for subject_data in data])\n",
        "\n",
        "def compute_second_diff(data):\n",
        "    return np.mean(np.abs(np.diff(data, n=2, axis=0)), axis=0)\n",
        "\n",
        "second_diff_features = np.array([compute_second_diff(subject_data) for subject_data in data])\n",
        "\n",
        "combined_time_features = np.concatenate(\n",
        "    [mean_features, variance_features, first_diff_features, second_diff_features], axis=1)"
      ],
      "metadata": {
        "id": "g3VVuctumFv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(final_labels)\n",
        "\n",
        "concatenated_array_features = np.array(combined_time_features , dtype=np.float32)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(concatenated_array_features, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "classifiers = {\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Naive Bayes\": GaussianNB()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1\n",
        "    }\n",
        "\n",
        "    print(f\"Results for {name}:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"\")\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nSummary of Classifier Performance:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mub9wiG2u9Tx",
        "outputId": "e6212bd2-ca4f-4ced-9528-1e565257e747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for SVM:\n",
            "Accuracy: 0.3276\n",
            "Precision: 0.4190\n",
            "Recall: 0.3276\n",
            "F1 Score: 0.2659\n",
            "\n",
            "Results for Random Forest:\n",
            "Accuracy: 0.5448\n",
            "Precision: 0.5331\n",
            "Recall: 0.5448\n",
            "F1 Score: 0.5375\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:06:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for XGBoost:\n",
            "Accuracy: 0.5069\n",
            "Precision: 0.4898\n",
            "Recall: 0.5069\n",
            "F1 Score: 0.4973\n",
            "\n",
            "Results for Gradient Boosting:\n",
            "Accuracy: 0.5069\n",
            "Precision: 0.4976\n",
            "Recall: 0.5069\n",
            "F1 Score: 0.5004\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for AdaBoost:\n",
            "Accuracy: 0.3759\n",
            "Precision: 0.3895\n",
            "Recall: 0.3759\n",
            "F1 Score: 0.3759\n",
            "\n",
            "Results for K-Nearest Neighbors:\n",
            "Accuracy: 0.4690\n",
            "Precision: 0.4004\n",
            "Recall: 0.4690\n",
            "F1 Score: 0.4126\n",
            "\n",
            "Results for Logistic Regression:\n",
            "Accuracy: 0.3690\n",
            "Precision: 0.3866\n",
            "Recall: 0.3690\n",
            "F1 Score: 0.3253\n",
            "\n",
            "Results for Naive Bayes:\n",
            "Accuracy: 0.2069\n",
            "Precision: 0.3259\n",
            "Recall: 0.2069\n",
            "F1 Score: 0.1558\n",
            "\n",
            "\n",
            "Summary of Classifier Performance:\n",
            "                     Accuracy  Precision    Recall  F1 Score\n",
            "SVM                  0.327586   0.418963  0.327586  0.265877\n",
            "Random Forest        0.544828   0.533142  0.544828  0.537486\n",
            "XGBoost              0.506897   0.489777  0.506897  0.497305\n",
            "Gradient Boosting    0.506897   0.497636  0.506897  0.500430\n",
            "AdaBoost             0.375862   0.389461  0.375862  0.375865\n",
            "K-Nearest Neighbors  0.468966   0.400386  0.468966  0.412603\n",
            "Logistic Regression  0.368966   0.386636  0.368966  0.325339\n",
            "Naive Bayes          0.206897   0.325899  0.206897  0.155795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}